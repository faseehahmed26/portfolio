[{"content":"🔗 GitHub Description Architected and implemented a comprehensive end-to-end data engineering pipeline that connects multiple distributed systems to ingest, process, and store data in real-time.\nDesigned a containerized environment using Docker Compose to orchestrate eight different services including Apache Airflow, Apache Kafka, Apache Spark, and Cassandra, ensuring seamless communication between components.\nDeveloped a scheduled data extraction workflow using Apache Airflow to fetch random user data from external APIs, enabling consistent and reliable data ingestion.\nImplemented a robust streaming architecture with Apache Kafka and Zookeeper that handles the continuous flow of data between services while maintaining data integrity.\nBuilt a real-time processing layer using Apache Spark with master-worker architecture that transforms and enriches the streaming data before storage.\nCreated a scalable and fault-tolerant data storage solution with Cassandra, structuring the database schema to optimize for the specific query patterns of the application.\nEngineered custom connectors between Kafka and Spark using the Spark Streaming API to ensure smooth data transition between the messaging system and processing engine.\nConfigured a PostgreSQL backend for Apache Airflow to maintain workflow state, execution history, and configurations, enhancing the reliability of the scheduled jobs.\nDeveloped a modular and maintainable Python codebase that connects all components, handling errors gracefully and providing detailed logging throughout the pipeline.\nImplemented a monitoring solution for the pipeline using Kafka Control Center, enabling visual tracking of data throughput, bottlenecks, and system performance.\nTechnical Architecture The system consists of several key components working together:\nData Ingestion Layer: Apache Airflow DAGs schedule and execute API calls to fetch random user data Messaging Layer: Apache Kafka and Zookeeper process and queue the ingested data Processing Layer: Apache Spark performs transformations on the streaming data Storage Layer: Cassandra database stores the processed data for analytical queries All components run in isolated Docker containers, communicating through a dedicated network. This architecture enables horizontal scaling of each component independently, providing flexibility and resilience to the overall system.\nKey Learnings Building this pipeline provided deep insights into:\nDesigning distributed systems that maintain consistency across multiple components Handling real-time data streams with exactly-once processing guarantees Containerizing complex applications with proper networking and resource allocation Implementing fault-tolerance in each layer of the data pipeline Optimizing data flow to minimize latency and maximize throughput ","permalink":"https://faseehahmed26.github.io/portfolio/projects/kafkaspark/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/kafkaspark-processing-engine\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eArchitected and implemented a comprehensive end-to-end data engineering pipeline that connects multiple distributed systems to ingest, process, and store data in real-time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDesigned a containerized environment using Docker Compose to orchestrate eight different services including Apache Airflow, Apache Kafka, Apache Spark, and Cassandra, ensuring seamless communication between components.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped a scheduled data extraction workflow using Apache Airflow to fetch random user data from external APIs, enabling consistent and reliable data ingestion.\u003c/p\u003e","title":"KafkaSpark: Containerized User Data Processing Engine"},{"content":"🔗 GitHub Repository 🔗 TestPyPi Package Overview Prompt Manager is a versatile Python library designed to automate the creation, versioning, and management of prompts for AI applications. Developed to address the challenges of manual prompt management in dynamic projects like resume automation, this tool ensures consistency and saves valuable development time.\nKey Features Efficient Prompt Creation \u0026amp; Updates: Easily add new prompt versions and manage iterations. Built-In Version Control: Automatically track every change, ensuring transparency and consistency. Dynamic Variable Injection: Retrieve formatted prompts with variable placeholders replaced at runtime. Scalable Architecture: Organize prompts by project, making it effortless to manage hundreds of prompts across diverse applications. Use Cases Chatbot Development: Maintain and iterate conversation templates with minimal overhead. Resume Automation: Generate customized resumes and cover letters without manual prompt adjustments. Rapid AI Prototyping: Experiment with different prompt strategies to optimize the performance of LLM-driven applications. Technologies \u0026amp; Tools Language \u0026amp; Framework: Python Package Management: Published on TestPyPi for seamless integration Workflow Automation: Eliminates manual prompt management, boosting productivity and reducing errors Prompt Manager is an essential tool for AI developers seeking to streamline their workflow and focus on building robust, production-ready applications.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/prompt-manager/","summary":"\u003ch3 id=\"-github-repository\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Prompt-Manager\"\u003eGitHub Repository\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-testpypi-package\"\u003e🔗 \u003ca href=\"https://test.pypi.org/project/prompt-manager/0.1.0/\"\u003eTestPyPi Package\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003ePrompt Manager is a versatile Python library designed to automate the creation, versioning, and management of prompts for AI applications. Developed to address the challenges of manual prompt management in dynamic projects like resume automation, this tool ensures consistency and saves valuable development time.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEfficient Prompt Creation \u0026amp; Updates:\u003c/strong\u003e Easily add new prompt versions and manage iterations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuilt-In Version Control:\u003c/strong\u003e Automatically track every change, ensuring transparency and consistency.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic Variable Injection:\u003c/strong\u003e Retrieve formatted prompts with variable placeholders replaced at runtime.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScalable Architecture:\u003c/strong\u003e Organize prompts by project, making it effortless to manage hundreds of prompts across diverse applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"use-cases\"\u003eUse Cases\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eChatbot Development:\u003c/strong\u003e Maintain and iterate conversation templates with minimal overhead.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResume Automation:\u003c/strong\u003e Generate customized resumes and cover letters without manual prompt adjustments.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRapid AI Prototyping:\u003c/strong\u003e Experiment with different prompt strategies to optimize the performance of LLM-driven applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies--tools\"\u003eTechnologies \u0026amp; Tools\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLanguage \u0026amp; Framework:\u003c/strong\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePackage Management:\u003c/strong\u003e Published on TestPyPi for seamless integration\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWorkflow Automation:\u003c/strong\u003e Eliminates manual prompt management, boosting productivity and reducing errors\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePrompt Manager is an essential tool for AI developers seeking to streamline their workflow and focus on building robust, production-ready applications.\u003c/p\u003e","title":"Prompt Manager: Automate Your Prompt Workflow"},{"content":"🔗 GitHub Repository Project Overview This project is an end-to-end machine learning pipeline for predicting customer churn in the telecommunications industry. It seamlessly integrates data ingestion, preprocessing, model evaluation, and deployment, showcasing my expertise in building production-grade solutions.\nKey Skills Demonstrated Data Engineering \u0026amp; Ingestion:\nDesigned robust data pipelines using validated schemas and YAML configuration for data ingestion and validation.\nMachine Learning \u0026amp; Model Evaluation:\nImplemented comprehensive model evaluation techniques using MLflow to track experiments, parameters, and metrics.\nDeployment \u0026amp; CI/CD:\nAutomated the build, containerization, and deployment process using Docker and GitHub Actions, with deployment on AWS (EC2, ECR).\nCloud \u0026amp; DevOps Integration:\nConfigured AWS services and implemented continuous integration/continuous deployment (CI/CD) pipelines to ensure scalable and reliable project delivery.\nEnd-to-End Workflow Management:\nCombined data ingestion, model evaluation, and deployment into a cohesive framework, ensuring reproducibility and traceability across the entire ML lifecycle.\nTechnologies \u0026amp; Tools Languages \u0026amp; Frameworks: Python, Flask, Docker ML \u0026amp; Data Tools: MLflow, Pandas, YAML, Statsmodels Cloud \u0026amp; Deployment: AWS (EC2, ECR), GitHub Actions, CI/CD pipelines Version Control: Git and GitHub This project exemplifies my ability to tackle complex data science challenges and deliver scalable, production-ready solutions. Whether it\u0026rsquo;s fine-tuning models, tracking experiments, or automating deployments, I bring a full spectrum of technical expertise to every project.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/telco-customer-churn/","summary":"\u003ch3 id=\"-github-repository\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/telco-customer-churn-project\"\u003eGitHub Repository\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"project-overview\"\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eThis project is an end-to-end machine learning pipeline for predicting customer churn in the telecommunications industry. It seamlessly integrates data ingestion, preprocessing, model evaluation, and deployment, showcasing my expertise in building production-grade solutions.\u003c/p\u003e\n\u003ch2 id=\"key-skills-demonstrated\"\u003eKey Skills Demonstrated\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Engineering \u0026amp; Ingestion:\u003c/strong\u003e\u003cbr\u003e\nDesigned robust data pipelines using validated schemas and YAML configuration for data ingestion and validation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMachine Learning \u0026amp; Model Evaluation:\u003c/strong\u003e\u003cbr\u003e\nImplemented comprehensive model evaluation techniques using MLflow to track experiments, parameters, and metrics.\u003c/p\u003e","title":"Telco Customer Churn Project"},{"content":"🔗 GitHub Repository Overview This project uses ARIMA and SARIMA models to forecast time series data. It includes:\nAirline Passengers Data (1949–1960) Tesla Stock Data Techniques covered:\nData cleaning and preprocessing Stationarity testing and differencing Model fitting, prediction, and visualization ","permalink":"https://faseehahmed26.github.io/portfolio/projects/airline-passenger-forecasting/","summary":"\u003ch5 id=\"-github-repository\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/airline-passenger-forecasting-using-arima-sarima\"\u003eGitHub Repository\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis project uses ARIMA and SARIMA models to forecast time series data. It includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAirline Passengers Data\u003c/strong\u003e (1949–1960)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTesla Stock Data\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTechniques covered:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eData cleaning and preprocessing\u003c/li\u003e\n\u003cli\u003eStationarity testing and differencing\u003c/li\u003e\n\u003cli\u003eModel fitting, prediction, and visualization\u003c/li\u003e\n\u003c/ul\u003e","title":"Airline Passenger Forecasting Time Series Forecasting"},{"content":"🔗 GitHub Description I developed a comprehensive web application for early detection of Chronic Kidney Disease (CKD) using machine learning algorithms. This healthcare tool addresses the critical need for timely CKD diagnosis, as early detection significantly improves treatment outcomes for a condition that often progresses silently until advanced stages.\nThe system utilizes the UCI Machine Learning Repository\u0026rsquo;s CKD dataset, which I preprocessed to handle the significant number of missing values typical in real-world medical data. I implemented an intelligent data completion algorithm that identifies the most similar complete samples to accurately fill missing values for each incomplete record.\nKey technical aspects include:\nAdvanced ML Implementation: Trained and evaluated multiple classification models, with Random Forest achieving exceptional 99.75% diagnostic accuracy Full-Stack Development: Built a complete MERN stack application with React frontend, Express/Node.js backend, and MongoDB database Intuitive User Interface: Designed a clear, accessible interface for healthcare professionals to input patient biomarkers and receive instant risk assessments Secure Data Handling: Implemented proper protocols for managing sensitive medical information in compliance with best practices The application allows clinicians to input various patient parameters such as blood pressure, blood glucose, albumin levels, and other key biomarkers to receive an immediate assessment of CKD risk. This tool serves as a valuable clinical decision support system, helping healthcare providers identify at-risk patients who might benefit from earlier intervention and specialized care.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/chronic-kidney/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Kidney-Detection\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI developed a comprehensive web application for early detection of Chronic Kidney Disease (CKD) using machine learning algorithms. This healthcare tool addresses the critical need for timely CKD diagnosis, as early detection significantly improves treatment outcomes for a condition that often progresses silently until advanced stages.\u003c/p\u003e\n\u003cp\u003eThe system utilizes the UCI Machine Learning Repository\u0026rsquo;s CKD dataset, which I preprocessed to handle the significant number of missing values typical in real-world medical data. I implemented an intelligent data completion algorithm that identifies the most similar complete samples to accurately fill missing values for each incomplete record.\u003c/p\u003e","title":"Chronic Kidney Disease Predictor"},{"content":"🔗 GitHub 🔗 Try the Bot Description I developed an intelligent COVID-19 chatbot deployed on Telegram that provides users with critical pandemic-related information and assistance. This AI-powered solution addresses the need for accessible, accurate, and timely COVID-19 information during a global health crisis.\nThe chatbot leverages Google DialogFlow\u0026rsquo;s natural language processing capabilities to understand user queries and provide appropriate responses across multiple categories:\nReal-time COVID-19 statistics by region using RapidAPI integration Vaccination appointment scheduling based on user preferences Critical resource location (oxygen supplies, plasma donations) Latest government announcements and health guidelines Frequently asked questions about COVID-19 News updates and preventive measures From a technical perspective, the project features a sophisticated architecture with DialogFlow handling natural language understanding, Flask powering the backend logic, MongoDB storing user data and preferences, and seamless integration with Telegram\u0026rsquo;s Bot API for message delivery.\nA key advantage of this implementation is its multilingual support, making critical health information accessible to non-English speakers—particularly important in diverse regions where English is not the primary language. The system was designed with scalability in mind, allowing for easy expansion of features and language support as pandemic information evolved.\nThis project demonstrates practical application of natural language processing and API integration to create a useful tool during a public health emergency, providing timely information and assistance when traditional information channels were overwhelmed.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/covid-19-bot/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Covid19-Bot\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-try-the-bot\"\u003e🔗 \u003ca href=\"https://t.me/covid_fabs_bot\"\u003eTry the Bot\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI developed an intelligent COVID-19 chatbot deployed on Telegram that provides users with critical pandemic-related information and assistance. This AI-powered solution addresses the need for accessible, accurate, and timely COVID-19 information during a global health crisis.\u003c/p\u003e\n\u003cp\u003eThe chatbot leverages Google DialogFlow\u0026rsquo;s natural language processing capabilities to understand user queries and provide appropriate responses across multiple categories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReal-time COVID-19 statistics by region using RapidAPI integration\u003c/li\u003e\n\u003cli\u003eVaccination appointment scheduling based on user preferences\u003c/li\u003e\n\u003cli\u003eCritical resource location (oxygen supplies, plasma donations)\u003c/li\u003e\n\u003cli\u003eLatest government announcements and health guidelines\u003c/li\u003e\n\u003cli\u003eFrequently asked questions about COVID-19\u003c/li\u003e\n\u003cli\u003eNews updates and preventive measures\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFrom a technical perspective, the project features a sophisticated architecture with DialogFlow handling natural language understanding, Flask powering the backend logic, MongoDB storing user data and preferences, and seamless integration with Telegram\u0026rsquo;s Bot API for message delivery.\u003c/p\u003e","title":"COVID-19 Bot"},{"content":"🔗 View the Tableau Dashboard Overview This interactive Tableau dashboard offers a comprehensive look at Twitter metrics, including tweet impressions, engagement rates, and volume by weekday. By visualizing tweet activity and user engagement, it helps optimize social media strategies for maximum impact.\nKey Features Tweet Performance\nTrack total impressions, average impression rates, and engagement rates at a glance.\nTop Tweets\nIdentify which tweets resonated most with your audience based on likes, retweets, or link clicks.\nTime-Based Trends\nSpot patterns by day of the week or time of day to schedule tweets more effectively.\nInteractive Visualization\nEasily filter by date range or day of the week to dig deeper into specific campaigns or content themes.\nTechnology Stack Data Visualization: Tableau Data Source: Twitter API exports or third-party analytics tools Key Visual Elements: Bar charts and line graphs for impression and engagement trends Dot matrix for day-by-day analysis Filter-based interactivity for custom exploration Use Cases Social Media Managers: Refine content strategies and posting schedules for better audience engagement. Marketing Teams: Evaluate campaign performance and identify best-performing tweet formats. Data Analysts: Understand patterns and correlations in tweet metrics to drive data-driven decisions. Explore the dashboard to discover how data-driven insights can elevate your Twitter presence and engagement.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/twitter-dashboard/","summary":"\u003ch3 id=\"-view-the-tableau-dashboard\"\u003e🔗 \u003ca href=\"https://public.tableau.com/views/KPIDashboardForTwitter/Dashboard2?:language=en-US\u0026amp;:sid=\u0026amp;:redirect=auth\u0026amp;:display_count=n\u0026amp;:origin=viz_share_link\"\u003eView the Tableau Dashboard\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis interactive Tableau dashboard offers a comprehensive look at Twitter metrics, including tweet impressions, engagement rates, and volume by weekday. By visualizing tweet activity and user engagement, it helps optimize social media strategies for maximum impact.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTweet Performance\u003c/strong\u003e\u003cbr\u003e\nTrack total impressions, average impression rates, and engagement rates at a glance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTop Tweets\u003c/strong\u003e\u003cbr\u003e\nIdentify which tweets resonated most with your audience based on likes, retweets, or link clicks.\u003c/p\u003e","title":"Twitter Dashboard"},{"content":"🔗 View the Tableau Dashboard Overview This interactive Tableau dashboard provides insights into the Google Play Store ecosystem, analyzing app ratings, categories, and installation trends. By segmenting data by rating filters, content category, and user reviews, it helps identify top-performing apps and emerging market opportunities.\nKey Features Ratings \u0026amp; Installs Overview\nQuickly see how apps rank based on average ratings and install counts, highlighting success stories.\nCategory Analysis\nCompare app performance across multiple categories—such as productivity, social, or games—to spot competitive gaps.\nUser Engagement \u0026amp; Reviews\nExplore content ratings (e.g., Everyone, 10+, Teen) and average reviews to understand user sentiment.\nFilter-Based Interactivity\nDrill down by Android version, rating filters, or content rating for a granular view of app performance metrics.\nTechnology Stack Data Visualization: Tableau Data Source: Google Play Store data (scraped or exported) Key Visual Elements: Tree maps for content rating distributions Bar charts highlighting top apps by installs or rating Filter controls for dynamic exploration Use Cases Developers \u0026amp; Product Managers: Discover which app categories thrive and how content rating affects user adoption. Marketing \u0026amp; Growth Teams: Identify top-performing apps and features to refine marketing strategies. Analysts \u0026amp; Researchers: Track trends in user reviews and installs, shaping data-driven product roadmaps. Explore the dashboard to uncover market insights and opportunities in the Google Play Store’s competitive app landscape.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/google-play-store/","summary":"\u003ch3 id=\"-view-the-tableau-dashboard\"\u003e🔗 \u003ca href=\"https://public.tableau.com/views/GooglePlaystoreDashBoard/Dashboard1?:language=en-US\u0026amp;:sid=\u0026amp;:redirect=auth\u0026amp;:display_count=n\u0026amp;:origin=viz_share_link\"\u003eView the Tableau Dashboard\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis interactive Tableau dashboard provides insights into the Google Play Store ecosystem, analyzing app ratings, categories, and installation trends. By segmenting data by rating filters, content category, and user reviews, it helps identify top-performing apps and emerging market opportunities.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRatings \u0026amp; Installs Overview\u003c/strong\u003e\u003cbr\u003e\nQuickly see how apps rank based on average ratings and install counts, highlighting success stories.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCategory Analysis\u003c/strong\u003e\u003cbr\u003e\nCompare app performance across multiple categories—such as productivity, social, or games—to spot competitive gaps.\u003c/p\u003e","title":"Google Play Store Apps Dashboard"},{"content":"🔗 GitHub 🔗 Live Demo 🔗 Dataset Description I built a real-time beverage inventory management system that combines computer vision with a database backend to automate stock tracking. The application uses YOLOv5 object detection to identify different beverage types (Coca-Cola, Sprite, Pepsi, etc.) through a camera feed, automatically updating inventory counts in a SQLite database.\nThe system features dual access modes - a staff interface for real-time scanning and an admin dashboard for inventory management. I implemented WebRTC for smooth live video streaming with detection speeds as low as 5ms per frame, ensuring the system could operate efficiently even on mobile devices.\nKey technical achievements include:\nCustom-trained YOLOv5 model optimized for beverage recognition with 90% accuracy Responsive Streamlit frontend with real-time video processing Automated database updates tracking inventory changes by date and product Support for both live camera detection and image upload workflows This project demonstrates practical application of deep learning in retail inventory management, significantly reducing manual counting time and improving accuracy. I also published an article about this project on Medium, explaining the implementation details and development process.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/beverage-managment-system/","summary":"\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Cold-Drinks-Inventory-System\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch5 id=\"-live-demo\"\u003e🔗 \u003ca href=\"https://faseehahmed26-cold-drinks-inventory-system-app-g8v3la.streamlit.app/\"\u003eLive Demo\u003c/a\u003e\u003c/h5\u003e\n\u003ch5 id=\"-dataset\"\u003e🔗 \u003ca href=\"https://www.kaggle.com/datasets/faseeh001/cold-drinks-inventory-dataset\"\u003eDataset\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI built a real-time beverage inventory management system that combines computer vision with a database backend to automate stock tracking. The application uses YOLOv5 object detection to identify different beverage types (Coca-Cola, Sprite, Pepsi, etc.) through a camera feed, automatically updating inventory counts in a SQLite database.\u003c/p\u003e\n\u003cp\u003eThe system features dual access modes - a staff interface for real-time scanning and an admin dashboard for inventory management. I implemented WebRTC for smooth live video streaming with detection speeds as low as 5ms per frame, ensuring the system could operate efficiently even on mobile devices.\u003c/p\u003e","title":"Beverage Management System"},{"content":"🔗 GitHub Description NutriPedia is a cutting-edge multimodal AI platform designed to empower users with smarter food recognition and detailed nutrition tracking, helping them make informed dietary choices effortlessly. By simply uploading a picture of a meal, users receive an accurate breakdown of portion size, caloric content, macronutrients, and vitamins—instantly and intelligently.\n🔍 Core Technology The system leverages a hybrid deep learning architecture combining Inception-V3 and Mask R-CNN to deliver high-precision food classification and segmentation.\nInception-V3: Known for its computational efficiency, it replaces traditional 5×5 convolutions with two 3×3 layers, reducing computational complexity by 33%. It achieves 93.7% classification accuracy on the FOOD-101 dataset. Mask R-CNN: Used for precise segmentation of individual food items, enabling accurate portion size estimation—a key factor for real-world calorie computation. ⚙️ Backend \u0026amp; Architecture NutriPedia includes a robust cloud-based infrastructure:\n🗂️ Google Cloud Storage for image storage 🧠 SQL databases for managing metadata and nutrition information 🔄 Flywheel Learning Mechanism: A self-reinforcing loop where user interactions continuously improve model accuracy over time 📊 Key Features Automated calorie estimation based on food type and portion size Seamless integration of computer vision with NLP via GPT-3.5 for natural, user-friendly summaries Scalable architecture designed to support thousands of food categories Currently supports 6 core food categories, with plans for broader dataset expansion 💬 Example Output (via GPT-3.5) “Your meal contains roughly 400 calories, with a healthy balance of carbohydrates and fiber. It’s a great post-workout option to restore energy levels and support digestion.”\n🎯 Real-World Impact NutriPedia addresses the growing global need for tools that support healthier lifestyles. As health awareness rises, so does the demand for smart, automated nutrition tracking. NutriPedia is poised to become a valuable companion for individuals aiming to monitor diet, prevent disordered eating, and maintain balanced nutrition—one photo at a time.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/nutripedia/","summary":"\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/nutripedia\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eNutriPedia\u003c/strong\u003e is a cutting-edge multimodal AI platform designed to empower users with smarter food recognition and detailed nutrition tracking, helping them make informed dietary choices effortlessly. By simply uploading a picture of a meal, users receive an accurate breakdown of portion size, caloric content, macronutrients, and vitamins—instantly and intelligently.\u003c/p\u003e\n\u003ch4 id=\"-core-technology\"\u003e🔍 Core Technology\u003c/h4\u003e\n\u003cp\u003eThe system leverages a hybrid deep learning architecture combining \u003cstrong\u003eInception-V3\u003c/strong\u003e and \u003cstrong\u003eMask R-CNN\u003c/strong\u003e to deliver high-precision food classification and segmentation.\u003c/p\u003e","title":"NutriPedia - Food Recognition \u0026 Calorie Estimation"},{"content":"🔗 GitHub 🔗 Live Demo Description I developed a machine learning application that accurately predicts used car prices based on various parameters, addressing the growing market demand for reliable pricing tools in the second-hand vehicle market. This end-to-end project combines data science and web development to create an accessible user interface for price estimation.\nThe development process followed a comprehensive workflow:\nData Cleaning - Processed the CarDekho dataset by handling missing values, removing duplicates, and standardizing formats to ensure data quality.\nExploratory Data Analysis - Uncovered key relationships between vehicle features and pricing using visualization tools (Seaborn, Matplotlib) to identify the most influential factors.\nFeature Engineering - Created new attributes and transformed existing ones to improve model performance, including categorical encoding and feature scaling.\nModel Training - Implemented and compared multiple regression algorithms (Linear Regression, Random Forest, XGBoost), ultimately selecting Random Forest Regression for its superior prediction accuracy.\nWeb Application Development - Built an intuitive user interface using Flask, HTML, CSS, and Bootstrap, allowing users to input vehicle specifications and receive instant price estimates.\nThis project demonstrates practical application of machine learning in the automotive industry, helping both buyers and sellers make informed decisions about used car valuations with a user-friendly web interface.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/second-hand-car/","summary":"\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Second-Hand-Car-Price\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch5 id=\"-live-demo\"\u003e🔗 \u003ca href=\"https://car-price-predictor-app.herokuapp.com/\"\u003eLive Demo\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI developed a machine learning application that accurately predicts used car prices based on various parameters, addressing the growing market demand for reliable pricing tools in the second-hand vehicle market. This end-to-end project combines data science and web development to create an accessible user interface for price estimation.\u003c/p\u003e\n\u003cp\u003eThe development process followed a comprehensive workflow:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Cleaning\u003c/strong\u003e - Processed the CarDekho dataset by handling missing values, removing duplicates, and standardizing formats to ensure data quality.\u003c/p\u003e","title":"Car Price Predictor"},{"content":"🔗 GitHub 🔗 Scraper Code Description I developed a comprehensive commodities analysis system that combines web scraping, data visualization, and deep learning to predict price movements across multiple markets. The project focused on six major commodities: Gold, Silver, Crude Oil, Brent Oil, Natural Gas, and Copper.\nFirst, I created a custom web scraper using Beautiful Soup to extract historical price data from Investing.com, allowing flexible date range selection. This data collection tool enabled me to compile 10 years of daily price information for analysis.\nThrough extensive exploratory data analysis, I identified significant cross-commodity correlations:\nStrong positive correlation between Crude Oil, Brent Oil, and Natural Gas prices Positive correlation between Gold, Silver, and Copper prices Interconnected relationships between energy and metals markets For the predictive component, I implemented Long Short-Term Memory (LSTM) neural networks using TensorFlow and Keras to forecast future price movements. The models achieved impressive accuracy with low Mean Squared Error scores across all commodities (Gold: 0.27, Silver: 0.40, Crude Oil: 0.34, Brent Oil: 0.27, Natural Gas: 0.35, Copper: 0.32).\nThis project demonstrates advanced abilities in time series analysis, web scraping, data visualization, and deep learning implementation. The insights derived provide valuable market intelligence for investors and commodity traders, while the LSTM models offer a foundation for algorithmic trading strategies.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/commodities/","summary":"\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Commodities-Investing.com\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch5 id=\"-scraper-code\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Commodities-Investing.com/blob/main/commoditiesScraper.py\"\u003eScraper Code\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI developed a comprehensive commodities analysis system that combines web scraping, data visualization, and deep learning to predict price movements across multiple markets. The project focused on six major commodities: Gold, Silver, Crude Oil, Brent Oil, Natural Gas, and Copper.\u003c/p\u003e\n\u003cp\u003eFirst, I created a custom web scraper using Beautiful Soup to extract historical price data from Investing.com, allowing flexible date range selection. This data collection tool enabled me to compile 10 years of daily price information for analysis.\u003c/p\u003e","title":"Analysis on Various Commodities"},{"content":"Description Developed a conversation-based Data Quality Tool (DQT) using RAG architecture and Mistral 7B, enabling data analysts to query clinical trial data in natural language with 90% accuracy and reducing analysis time by 45%. Built a modular GenAI system with three configurable RAG pipelines, optimizing for different use cases: fast prototyping, balanced production use, and high-precision regulatory reporting. Engineered an intelligent document processing system using BERT-based models for ICF classification, implementing weak supervision techniques that reduced manual labeling by 70% while maintaining high data quality. Designed data pipelines integrating clinical trial data from Veeva Vault to Redshift data warehouse, creating optimized data models that improved query performance for Tableau dashboard builders by 40%. Fine-tuned specialized LLMs for clinical trial domain knowledge using LoRA, increasing classification F1-scores from 0.58 to 0.84 and reducing hallucinations by 65% in RAG responses. Created a comprehensive evaluation framework benchmarking different embedding models (all-MiniLM-L6-v2, all-mpnet-base-v2, intfloat/e5-large) with corresponding vector stores (FAISS, HNSW, Chroma), enabling data teams to select optimal configurations for their specific use cases. Developed an ETL workflow automation using Airflow and SQLAlchemy that ensured data integrity between source systems and analytical platforms, reducing manual intervention by 80%. Collaborated with Tableau developers to implement a hybrid analytics approach combining traditional dashboards with conversational AI interfaces, allowing non-technical stakeholders to access insights directly. Applied advanced NLP techniques including semantic search, text clustering with BERTopic, and chunk optimization strategies that improved information retrieval precision by 35% for clinical trial documentation. Built an end-to-end LLM deployment pipeline with model quantization (GGUF) and serving optimization (vLLM), reducing inference latency by 62% while maintaining response quality for production environments. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/daiichi-sankyo/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a conversation-based Data Quality Tool (DQT) using RAG architecture and Mistral 7B, enabling data analysts to query clinical trial data in natural language with 90% accuracy and reducing analysis time by 45%.\u003c/li\u003e\n\u003cli\u003eBuilt a modular GenAI system with three configurable RAG pipelines, optimizing for different use cases: fast prototyping, balanced production use, and high-precision regulatory reporting.\u003c/li\u003e\n\u003cli\u003eEngineered an intelligent document processing system using BERT-based models for ICF classification, implementing weak supervision techniques that reduced manual labeling by 70% while maintaining high data quality.\u003c/li\u003e\n\u003cli\u003eDesigned data pipelines integrating clinical trial data from Veeva Vault to Redshift data warehouse, creating optimized data models that improved query performance for Tableau dashboard builders by 40%.\u003c/li\u003e\n\u003cli\u003eFine-tuned specialized LLMs for clinical trial domain knowledge using LoRA, increasing classification F1-scores from 0.58 to 0.84 and reducing hallucinations by 65% in RAG responses.\u003c/li\u003e\n\u003cli\u003eCreated a comprehensive evaluation framework benchmarking different embedding models (all-MiniLM-L6-v2, all-mpnet-base-v2, intfloat/e5-large) with corresponding vector stores (FAISS, HNSW, Chroma), enabling data teams to select optimal configurations for their specific use cases.\u003c/li\u003e\n\u003cli\u003eDeveloped an ETL workflow automation using Airflow and SQLAlchemy that ensured data integrity between source systems and analytical platforms, reducing manual intervention by 80%.\u003c/li\u003e\n\u003cli\u003eCollaborated with Tableau developers to implement a hybrid analytics approach combining traditional dashboards with conversational AI interfaces, allowing non-technical stakeholders to access insights directly.\u003c/li\u003e\n\u003cli\u003eApplied advanced NLP techniques including semantic search, text clustering with BERTopic, and chunk optimization strategies that improved information retrieval precision by 35% for clinical trial documentation.\u003c/li\u003e\n\u003cli\u003eBuilt an end-to-end LLM deployment pipeline with model quantization (GGUF) and serving optimization (vLLM), reducing inference latency by 62% while maintaining response quality for production environments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/daiichi-sankyo/download.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"R\u0026D Data Governance Intern"},{"content":"🔗 Publication Abstract Research Overview I presented our groundbreaking research on tubule detection in breast carcinoma whole slide images at SIIM-CMIMI22 (International Conference On Machine Intelligence in Medical Imaging) at Johns Hopkins Hospital, Baltimore, MD. This work addresses a critical component of the Nottingham Breast Cancer Grading system, automating what has traditionally been a tedious manual process for pathologists.\nTechnical Approach Our system employed the R2U-Net CNN architecture, a specialized recurrent residual convolutional neural network optimized for medical image segmentation. The model was trained on two distinct datasets:\nPublic Dataset: 85 whole slide images (775 x 522) at 40x resolution containing 795 annotated tubules Local Hospital Dataset: 50 whole slide images (40,000 x 40,000) at 40x resolution with approximately 25,000 tubules from Basavatarakam Indo-American Cancer Hospital in Hyderabad, India Distributed Training Innovation A key innovation in our approach was implementing distributed training using Horovod, an open-source framework that leverages data parallelism to optimize resource allocation:\nTrained the model across 4 GPUs simultaneously Achieved 3.9x faster training compared to single-GPU implementation Reduced training time from ~27 hours to just 7 hours for 500 epochs System Architecture The end-to-end pipeline consisted of:\nData Preparation: Pathologists annotated tubules using CADD4MBC (Computer-Aided Detection and Diagnosis for Molecular Breast Cancer), with annotations saved as JSON files Preprocessing: Custom scripts using QuPath and Groovy to extract and prepare training data Model Training: R2U-Net architecture with distributed training via Horovod Integration: Web-based viewer for pathologists to upload, analyze, and review results Results Our model achieved exceptional performance metrics:\nDataset Training F1 Score Training Mean IOU Testing F1 Score Testing Mean IOU Public 0.9974 0.9070 0.9293 0.7537 KMIT 0.9962 0.9116 0.9105 0.8053 These results demonstrate that deep learning approaches can effectively automate tubule detection with high accuracy, potentially reducing pathologists\u0026rsquo; workload and improving diagnostic consistency in breast cancer grading.\nImpact This research represents a significant advancement in computational pathology, offering:\nFaster and more consistent tubule detection for breast cancer grading Reduced burden on pathologists for manual identification A scalable framework that can be extended to other histopathological features Integration with existing clinical workflows through web-based platforms The project demonstrates how deep learning can enhance cancer diagnostics while maintaining the critical role of pathologists in the evaluation process. ","permalink":"https://faseehahmed26.github.io/portfolio/publications/tubules-detection/","summary":"\u003ch3 id=\"-publication-abstract\"\u003e🔗 \u003ca href=\"https://cdn.ymaws.com/siim.org/resource/resmgr/mimi22/abstracts/Tubules_Detection_on_Breast_.pdf\"\u003ePublication Abstract\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"research-overview\"\u003eResearch Overview\u003c/h2\u003e\n\u003cp\u003eI presented our groundbreaking research on tubule detection in breast carcinoma whole slide images at SIIM-CMIMI22 (International Conference On Machine Intelligence in Medical Imaging) at Johns Hopkins Hospital, Baltimore, MD. This work addresses a critical component of the Nottingham Breast Cancer Grading system, automating what has traditionally been a tedious manual process for pathologists.\u003c/p\u003e\n\u003ch2 id=\"technical-approach\"\u003eTechnical Approach\u003c/h2\u003e\n\u003cp\u003eOur system employed the R2U-Net CNN architecture, a specialized recurrent residual convolutional neural network optimized for medical image segmentation. The model was trained on two distinct datasets:\u003c/p\u003e","title":"Tubules Detection on Breast Carcinoma Whole Slide Images"},{"content":"Description Pioneered advanced federated learning architectures by transitioning from request-based to gRPC communication protocols, significantly enhancing cross-device model synchronization and security. Led implementation of the FedDisco algorithm within distributed learning frameworks, reducing model convergence time by 20% and enabling adaptive communication strategies for real-time updates. Engineered specialized distributed training techniques for Large Language Models (LLMs), achieving a 30% performance boost while minimizing network latency and resource consumption. Optimized PyTorch RPC-based distributed environments, reducing communication overhead by 15% through advanced data transfer protocol refinements and node orchestration. Developed a production-ready React Native application enabling on-device federated learning with TensorFlow.js, making sophisticated AI training accessible on resource-constrained mobile devices. Designed and implemented custom secure communication channels using gRPC for sensitive data synchronization, improving overall model accuracy by 10% while maintaining strict privacy constraints. Collaborated directly with Prof. Haibo Yang to enhance the theoretical foundations and practical implementations of federated learning frameworks for edge computing applications. Created and optimized custom loss functions and training regimes tailored for inconsistent network conditions, improving model robustness in real-world distributed environments. Mentored graduate students in advanced distributed systems concepts, providing technical guidance that significantly improved their implementation quality and research outcomes. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/rit/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePioneered advanced federated learning architectures by transitioning from request-based to gRPC communication protocols, significantly enhancing cross-device model synchronization and security.\u003c/li\u003e\n\u003cli\u003eLed implementation of the FedDisco algorithm within distributed learning frameworks, reducing model convergence time by 20% and enabling adaptive communication strategies for real-time updates.\u003c/li\u003e\n\u003cli\u003eEngineered specialized distributed training techniques for Large Language Models (LLMs), achieving a 30% performance boost while minimizing network latency and resource consumption.\u003c/li\u003e\n\u003cli\u003eOptimized PyTorch RPC-based distributed environments, reducing communication overhead by 15% through advanced data transfer protocol refinements and node orchestration.\u003c/li\u003e\n\u003cli\u003eDeveloped a production-ready React Native application enabling on-device federated learning with TensorFlow.js, making sophisticated AI training accessible on resource-constrained mobile devices.\u003c/li\u003e\n\u003cli\u003eDesigned and implemented custom secure communication channels using gRPC for sensitive data synchronization, improving overall model accuracy by 10% while maintaining strict privacy constraints.\u003c/li\u003e\n\u003cli\u003eCollaborated directly with Prof. Haibo Yang to enhance the theoretical foundations and practical implementations of federated learning frameworks for edge computing applications.\u003c/li\u003e\n\u003cli\u003eCreated and optimized custom loss functions and training regimes tailored for inconsistent network conditions, improving model robustness in real-world distributed environments.\u003c/li\u003e\n\u003cli\u003eMentored graduate students in advanced distributed systems concepts, providing technical guidance that significantly improved their implementation quality and research outcomes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/rit/rit_home.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Research Assistant"},{"content":"Normal\n🔗 Publication Link Research Summary This research, published in the International Journal of Advanced Computer Science Applications (IJACSA), presents an end-to-end web-based deep learning system for detecting mitotic figures in breast cancer whole slide images (WSI). Mitotic count is a critical component of the Nottingham Breast Cancer Grading system, but manual counting is time-consuming and subject to inter-observer variability.\nModel Comparison We implemented and compared two state-of-the-art object detection architectures:\nFasterRCNN: A two-stage detector implemented with PyTorch and Detectron2 YOLOv5: A single-stage detector known for its speed and accuracy Our comparative analysis focused on both accuracy and computational efficiency to identify the most practical solution for clinical deployment.\nDataset and Training The training process utilized:\n56,258 annotated tiles extracted from whole slide images 45,006 tiles for training and 11,251 for testing High-resolution input (40x magnification) Rigorous data augmentation to improve model generalization Performance Analysis The YOLOv5 model demonstrated superior performance across all key metrics:\nModel F1 Score Training Time Precision Recall FasterRCNN 77% 10h 46m 73% 81% YOLOv5 84% 5h 28m 82% 86% This represents both higher accuracy and approximately half the training time, making YOLOv5 the preferred model for this application.\nWeb Platform Integration A key innovation of our work was the integration with CADD4MBC, a web-based digital pathology platform that enables:\nWSI upload and management Annotation tools for ground truth creation Automatic mitosis detection using our trained models Result visualization and validation Detection Results Our system successfully detected mitotic figures in complex histopathological images with high accuracy, distinguishing them from similar-appearing structures that often confuse human observers.\nClinical Impact This research makes several important contributions to computational pathology:\nReducing the time required for mitosis counting from hours to minutes Improving consistency and reducing inter-observer variability Providing an accessible web-based platform that integrates with existing clinical workflows Demonstrating the superiority of YOLOv5 for this specific medical imaging application The web-based nature of our solution makes it particularly valuable for remote consultations and collaborative diagnostics, potentially expanding access to expert pathology services.\n","permalink":"https://faseehahmed26.github.io/portfolio/publications/mitosis-detection/","summary":"\u003cp\u003e\u003cuserStyle\u003eNormal\u003c/userStyle\u003e\u003c/p\u003e\n\u003ch3 id=\"-publication-link\"\u003e🔗 \u003ca href=\"https://thesai.org/Publications/ViewPaper?Volume=13\u0026amp;Issue=10\u0026amp;Code=IJACSA\u0026amp;SerialNo=62\"\u003ePublication Link\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"research-summary\"\u003eResearch Summary\u003c/h2\u003e\n\u003cp\u003eThis research, published in the International Journal of Advanced Computer Science Applications (IJACSA), presents an end-to-end web-based deep learning system for detecting mitotic figures in breast cancer whole slide images (WSI). Mitotic count is a critical component of the Nottingham Breast Cancer Grading system, but manual counting is time-consuming and subject to inter-observer variability.\u003c/p\u003e\n\u003ch2 id=\"model-comparison\"\u003eModel Comparison\u003c/h2\u003e\n\u003cp\u003eWe implemented and compared two state-of-the-art object detection architectures:\u003c/p\u003e","title":"Web-based Mitosis Detection on Breast Cancer WSI"},{"content":"🔗 GitHub Repository Description Engineered a comprehensive data pipeline to scrape, process, and analyze 3,100+ Bollywood movies from IMDB using Python, BeautifulSoup, and Requests.\nImplemented robust web scraping algorithms to extract detailed movie information including cast, plot summaries, keywords, release dates, genres, and user reviews from multiple IMDB pages.\nBuilt data cleaning and transformation workflows that structured unformatted HTML content into a clean, analysis-ready dataset with 11 normalized features.\nDeveloped automated data enrichment processes to calculate derived metrics including content age, plot keyword extraction, and temporal analysis of movie releases.\nCreated error handling mechanisms to manage inconsistent HTML structures, missing data points, and rate limiting during the scraping process.\nOptimized the scraping workflow by implementing randomized delays and batch processing, successfully extracting data from 4,000 movie search queries.\nConstructed a scalable ETL pipeline that transformed raw HTML into structured data stored in CSV format for downstream recommendation algorithms.\nIntegrated metadata analysis techniques to identify patterns in movie genres, plot elements, and cast relationships across Bollywood cinema.\nDesigned the foundation for a recommendation system utilizing content-based filtering based on movie metadata and similarity metrics.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/bollywood-movie-recommender/","summary":"\u003ch5 id=\"-github-repository\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Movie-Recommender-System\"\u003eGitHub Repository\u003c/a\u003e\u003c/h5\u003e\n\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEngineered a comprehensive data pipeline to scrape, process, and analyze 3,100+ Bollywood movies from IMDB using Python, BeautifulSoup, and Requests.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented robust web scraping algorithms to extract detailed movie information including cast, plot summaries, keywords, release dates, genres, and user reviews from multiple IMDB pages.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuilt data cleaning and transformation workflows that structured unformatted HTML content into a clean, analysis-ready dataset with 11 normalized features.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped automated data enrichment processes to calculate derived metrics including content age, plot keyword extraction, and temporal analysis of movie releases.\u003c/p\u003e","title":"Bollywood Movie Recommender System"},{"content":"🔗 GitHub 🔗 Live Demo Description Developed a machine learning system that predicts domestic flight prices with ~80% accuracy using a Random Forest Regression model, enabling travelers to make cost-effective booking decisions.\nImplemented comprehensive data processing techniques for complex temporal features, extracting journey day, month, departure time, arrival time, and flight duration from raw string data.\nEngineered a sophisticated data preprocessing pipeline that handles categorical features through one-hot encoding for airlines, source, and destination, improving model performance.\nCreated custom feature extraction for duration data, splitting into hours and minutes components to provide more meaningful input signals for the model.\nApplied feature importance analysis using ExtraTreesRegressor to identify key price determinants, with total stops, journey day, and airline carrier emerging as the most influential factors.\nOptimized the Random Forest model through RandomizedSearchCV, systematically exploring 50 parameter combinations across 10 iterations to reduce prediction error.\nAchieved high model performance metrics with R² score of 0.797, Mean Absolute Error of 1,145 rupees, and Root Mean Square Error of 2,011 rupees on unseen test data.\nBuilt a user-friendly web interface using Flask that accepts inputs like departure location, destination, travel date, and airline, delivering instant fare predictions.\nDeployed the trained model as a web service using Heroku\u0026rsquo;s cloud platform, making the predictive analytics tool accessible to travelers worldwide.\nCreated insightful visualizations of price correlations across airlines, routes, and timing factors to uncover underlying patterns in the pricing data.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/flight-price-predictor/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Flight-Fare-Predictor\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-live-demo\"\u003e🔗 \u003ca href=\"https://flight-fare-predictor-app.herokuapp.com/\"\u003eLive Demo\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped a machine learning system that predicts domestic flight prices with ~80% accuracy using a Random Forest Regression model, enabling travelers to make cost-effective booking decisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented comprehensive data processing techniques for complex temporal features, extracting journey day, month, departure time, arrival time, and flight duration from raw string data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEngineered a sophisticated data preprocessing pipeline that handles categorical features through one-hot encoding for airlines, source, and destination, improving model performance.\u003c/p\u003e","title":"Flight Fare Predictor"},{"content":"Description Designed hands-on projects for the Intro to Machine Learning course, translating theoretical concepts into practical Python and PyTorch implementations that reinforced student understanding. Provided personalized technical support and mentorship to over 25 graduate students, significantly improving their grasp of complex distributed machine learning concepts and implementation techniques. Created comprehensive tutorial materials on distributed computing paradigms, helping students navigate challenging topics including parameter server architectures and decentralized training. Led weekly lab sessions focusing on practical machine learning implementations, guiding students through model development, training optimization, and performance evaluation. Developed and graded assessment materials that effectively evaluated student understanding of core machine learning concepts and their ability to implement solutions independently. Facilitated student research projects, helping them formulate hypotheses, design experiments, and analyze results using appropriate statistical methods and visualization techniques. Collaborated with faculty to refine course curriculum, incorporating the latest advances in machine learning and distributed computing to keep content relevant and cutting-edge. Organized and moderated peer learning sessions where students could collaborate on complex problems, fostering a supportive learning environment that improved overall class performance. Implemented a structured feedback system that identified common learning obstacles, allowing for targeted intervention that reduced assignment resubmission rates by 30%. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/rit_1/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDesigned hands-on projects for the Intro to Machine Learning course, translating theoretical concepts into practical Python and PyTorch implementations that reinforced student understanding.\u003c/li\u003e\n\u003cli\u003eProvided personalized technical support and mentorship to over 25 graduate students, significantly improving their grasp of complex distributed machine learning concepts and implementation techniques.\u003c/li\u003e\n\u003cli\u003eCreated comprehensive tutorial materials on distributed computing paradigms, helping students navigate challenging topics including parameter server architectures and decentralized training.\u003c/li\u003e\n\u003cli\u003eLed weekly lab sessions focusing on practical machine learning implementations, guiding students through model development, training optimization, and performance evaluation.\u003c/li\u003e\n\u003cli\u003eDeveloped and graded assessment materials that effectively evaluated student understanding of core machine learning concepts and their ability to implement solutions independently.\u003c/li\u003e\n\u003cli\u003eFacilitated student research projects, helping them formulate hypotheses, design experiments, and analyze results using appropriate statistical methods and visualization techniques.\u003c/li\u003e\n\u003cli\u003eCollaborated with faculty to refine course curriculum, incorporating the latest advances in machine learning and distributed computing to keep content relevant and cutting-edge.\u003c/li\u003e\n\u003cli\u003eOrganized and moderated peer learning sessions where students could collaborate on complex problems, fostering a supportive learning environment that improved overall class performance.\u003c/li\u003e\n\u003cli\u003eImplemented a structured feedback system that identified common learning obstacles, allowing for targeted intervention that reduced assignment resubmission rates by 30%.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/rit/rit_home.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Teaching Assistant - Machine Learning"},{"content":"Description Architected an end-to-end data pipeline for AI-driven content generation, leveraging microservices to process, transform, and deliver SEO-optimized content at scale with 60x reduction in operational costs. Engineered a containerized NLP infrastructure using AWS ECS, Lambda, and API Gateway to dynamically select and deploy the optimal language model (GPT-3.5, BERT, FLAN-T5) based on content requirements and cost constraints. Designed a modular prompt engineering system with version control that enabled non-technical users to modify AI behavior without developer intervention, reducing engineering overhead by 40%. Built a Chrome extension using Python, JavaScript, and NodeJS that increased content generation speed by 40% while maintaining quality through automated SEO validation checks. Implemented robust data transformation workflows across the entire content lifecycle, from raw keyword extraction to structured, SEO-optimized output, ensuring consistent data quality at every stage. Developed automated keyword clustering algorithms using advanced NLP techniques, enabling the platform to generate topically-related article sets that improved organic search traffic by 30-35%. Constructed intelligent data processing pipelines that seamlessly integrated with third-party platforms (Shopify, WooCommerce, WordPress) via custom APIs, allowing direct content publishing without manual intervention. Created scalable ETL processes to ingest, transform, and analyze SEO performance data from multiple sources (Google Search Console, Ahrefs, SEMrush), forming a feedback loop that continuously improved content quality. Optimized infrastructure costs through strategic use of containerization, caching mechanisms, and serverless architectures, achieving a 35% reduction in cloud expenditure while handling increased processing volumes. Implemented comprehensive monitoring and alerting systems using CloudWatch and SNS, ensuring 99.9% API availability and rapid response to performance anomalies. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/seocontent-ai/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected an end-to-end data pipeline for AI-driven content generation, leveraging microservices to process, transform, and deliver SEO-optimized content at scale with 60x reduction in operational costs.\u003c/li\u003e\n\u003cli\u003eEngineered a containerized NLP infrastructure using AWS ECS, Lambda, and API Gateway to dynamically select and deploy the optimal language model (GPT-3.5, BERT, FLAN-T5) based on content requirements and cost constraints.\u003c/li\u003e\n\u003cli\u003eDesigned a modular prompt engineering system with version control that enabled non-technical users to modify AI behavior without developer intervention, reducing engineering overhead by 40%.\u003c/li\u003e\n\u003cli\u003eBuilt a Chrome extension using Python, JavaScript, and NodeJS that increased content generation speed by 40% while maintaining quality through automated SEO validation checks.\u003c/li\u003e\n\u003cli\u003eImplemented robust data transformation workflows across the entire content lifecycle, from raw keyword extraction to structured, SEO-optimized output, ensuring consistent data quality at every stage.\u003c/li\u003e\n\u003cli\u003eDeveloped automated keyword clustering algorithms using advanced NLP techniques, enabling the platform to generate topically-related article sets that improved organic search traffic by 30-35%.\u003c/li\u003e\n\u003cli\u003eConstructed intelligent data processing pipelines that seamlessly integrated with third-party platforms (Shopify, WooCommerce, WordPress) via custom APIs, allowing direct content publishing without manual intervention.\u003c/li\u003e\n\u003cli\u003eCreated scalable ETL processes to ingest, transform, and analyze SEO performance data from multiple sources (Google Search Console, Ahrefs, SEMrush), forming a feedback loop that continuously improved content quality.\u003c/li\u003e\n\u003cli\u003eOptimized infrastructure costs through strategic use of containerization, caching mechanisms, and serverless architectures, achieving a 35% reduction in cloud expenditure while handling increased processing volumes.\u003c/li\u003e\n\u003cli\u003eImplemented comprehensive monitoring and alerting systems using CloudWatch and SNS, ensuring 99.9% API availability and rapid response to performance anomalies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/seocontent-ai/images.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"AI Infrastructure Engineer"},{"content":"🔗 GitHub Description Developed a high-performance predictive model achieving 71.7% F1 score to identify factors influencing World Bank project success or closure, enabling better resource allocation decisions and risk assessment.\nEngineered a robust feature selection pipeline to handle complex hierarchical data with 17,500+ records and 27 variables, reducing dimensionality while preserving critical predictive signals.\nImplemented XGBoost classification with hyperparameter optimization via RandomizedSearchCV, systematically evaluating model performance across multiple parameter combinations.\nCreated data preprocessing workflows to address significant missing data (30%+ in key fields) and class imbalance (90/10 split), ensuring model generalizability and statistical validity.\nConducted in-depth exploratory data analysis uncovering relationships between regional, financial, and categorical variables that influence project outcomes across 200+ countries.\nDesigned custom evaluation metrics prioritizing F1 score over accuracy to account for class imbalance, providing stakeholders with realistic performance expectations.\nOptimized model parameters including learning rate, tree depth, and regularization techniques to prevent overfitting while maximizing predictive performance.\nImplemented an automated model persistence system with pickle, enabling seamless deployment for future predictions and analysis.\nDelivered comprehensive statistical analysis of model performance including precision (80.6%), recall (64.6%), and mean absolute error metrics for stakeholder interpretation.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/world-bank-project-analysis/","summary":"\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/World-Bank-Projects\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped a high-performance predictive model achieving 71.7% F1 score to identify factors influencing World Bank project success or closure, enabling better resource allocation decisions and risk assessment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEngineered a robust feature selection pipeline to handle complex hierarchical data with 17,500+ records and 27 variables, reducing dimensionality while preserving critical predictive signals.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented XGBoost classification with hyperparameter optimization via RandomizedSearchCV, systematically evaluating model performance across multiple parameter combinations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreated data preprocessing workflows to address significant missing data (30%+ in key fields) and class imbalance (90/10 split), ensuring model generalizability and statistical validity.\u003c/p\u003e","title":"World Bank Projects Analysis"},{"content":"Normal\n🔗 GitHub 🔗 Dataset Description Developed a deep learning audio classification system that detects Capuchin bird calls in rainforest recordings, achieving 100% recall and precision on validation data after only 4 training epochs.\nImplemented end-to-end audio processing pipeline using TensorFlow and TensorFlow IO, handling everything from raw WAV/MP3 files to model-ready spectrograms.\nCreated custom audio preprocessing functions to resample audio from 44.1kHz to 16kHz, normalize amplitudes, and generate time-frequency spectrograms using Short-Time Fourier Transform (STFT).\nEngineered an efficient data loading workflow for handling variable-length audio files by padding shorter clips and creating equal-length windows (48,000 samples) across longer recordings.\nBuilt and trained a CNN model architecture with two convolutional layers, max pooling, and dense layers optimized for spectrogram pattern recognition in audio signals.\nDesigned custom forest recording parsing functions to analyze extended MP3 audio files by splitting them into overlapping windows and making sequential predictions.\nImplemented post-processing algorithms using itertools to group consecutive positive detections, effectively counting distinct Capuchin calls in continuous forest recordings.\nCreated a scalable system that processes complete forest audio recordings to accurately count Capuchin bird calls, enabling wildlife conservation monitoring and research.\nOptimized the model to minimize false positives and achieved near-perfect accuracy in distinguishing Capuchin calls from other rainforest sounds in noisy environments.\nGenerated CSV outputs with processed results for visualization and analysis, along with a saved model artifact for deployment in field research applications.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/capuchin-audio-classifier/","summary":"\u003cp\u003e\u003cuserStyle\u003eNormal\u003c/userStyle\u003e\u003c/p\u003e\n\u003ch5 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/CapuchinClassifier\"\u003eGitHub\u003c/a\u003e\u003c/h5\u003e\n\u003ch5 id=\"-dataset\"\u003e🔗 \u003ca href=\"https://www.kaggle.com/code/duanboomer/capuchin-bird-audio-classification\"\u003eDataset\u003c/a\u003e\u003c/h5\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped a deep learning audio classification system that detects Capuchin bird calls in rainforest recordings, achieving 100% recall and precision on validation data after only 4 training epochs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented end-to-end audio processing pipeline using TensorFlow and TensorFlow IO, handling everything from raw WAV/MP3 files to model-ready spectrograms.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreated custom audio preprocessing functions to resample audio from 44.1kHz to 16kHz, normalize amplitudes, and generate time-frequency spectrograms using Short-Time Fourier Transform (STFT).\u003c/p\u003e","title":"Capuchin Bird Audio Classifier"},{"content":"Description Architected an end-to-end NLP pipeline that automated article generation for digital marketing, achieving a 60x reduction in operational costs through strategic implementation of AWS Lambda, NodeJS, and API Gateway. Engineered a scalable content generation system processing over 130,000 high-quality articles weekly, implementing distributed processing across multiple language models to balance computational load. Developed sophisticated model selection and orchestration layers to dynamically route content generation requests to the optimal LLM (BERT, RoBERTa, ALBERT, or DistilBERT) based on content complexity, topic, and quality requirements. Implemented advanced fine-tuning pipelines for multiple language models, achieving 42% improvement in contextual understanding and content relevance through targeted domain adaptation. Designed a comprehensive data lifecycle management system using DynamoDB, ensuring efficient data access patterns and optimized read/write operations for high-throughput content generation. Created modular prompt engineering frameworks that enabled non-technical users to customize content generation without engineering intervention, improving cross-team collaboration and reducing development bottlenecks. Built real-time content validation and quality assurance pipelines that performed automated checks for SEO compliance, readability, and brand tone adherence before delivery. Established an event-driven ETL architecture for keyword extraction and content clustering, enabling automatic identification of topic hierarchies and semantic relationships between generated articles. Implemented intelligent data transformation workflows that converted unstructured source materials into structured, SEO-optimized content through multi-stage NLP processing. Integrated comprehensive monitoring and analytics using CloudWatch and custom logging frameworks, providing real-time insights into model performance, content quality metrics, and system health. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/wlr/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected an end-to-end NLP pipeline that automated article generation for digital marketing, achieving a 60x reduction in operational costs through strategic implementation of AWS Lambda, NodeJS, and API Gateway.\u003c/li\u003e\n\u003cli\u003eEngineered a scalable content generation system processing over 130,000 high-quality articles weekly, implementing distributed processing across multiple language models to balance computational load.\u003c/li\u003e\n\u003cli\u003eDeveloped sophisticated model selection and orchestration layers to dynamically route content generation requests to the optimal LLM (BERT, RoBERTa, ALBERT, or DistilBERT) based on content complexity, topic, and quality requirements.\u003c/li\u003e\n\u003cli\u003eImplemented advanced fine-tuning pipelines for multiple language models, achieving 42% improvement in contextual understanding and content relevance through targeted domain adaptation.\u003c/li\u003e\n\u003cli\u003eDesigned a comprehensive data lifecycle management system using DynamoDB, ensuring efficient data access patterns and optimized read/write operations for high-throughput content generation.\u003c/li\u003e\n\u003cli\u003eCreated modular prompt engineering frameworks that enabled non-technical users to customize content generation without engineering intervention, improving cross-team collaboration and reducing development bottlenecks.\u003c/li\u003e\n\u003cli\u003eBuilt real-time content validation and quality assurance pipelines that performed automated checks for SEO compliance, readability, and brand tone adherence before delivery.\u003c/li\u003e\n\u003cli\u003eEstablished an event-driven ETL architecture for keyword extraction and content clustering, enabling automatic identification of topic hierarchies and semantic relationships between generated articles.\u003c/li\u003e\n\u003cli\u003eImplemented intelligent data transformation workflows that converted unstructured source materials into structured, SEO-optimized content through multi-stage NLP processing.\u003c/li\u003e\n\u003cli\u003eIntegrated comprehensive monitoring and analytics using CloudWatch and custom logging frameworks, providing real-time insights into model performance, content quality metrics, and system health.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/white-label-resell/wlr_home.jpg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Machine Learning Engineer"},{"content":"Description Engineered a sophisticated image classification system using Faster R-CNN for cancer cell detection in histopathological images, achieving 0.82 F1 score across 1,000 validated samples while optimizing model architecture for clinical efficiency. Developed a custom segmentation pipeline with Detectron2 that precisely delineated tumor boundaries in high-resolution medical scans, implementing a specialized loss function that improved detection accuracy by 20% on challenging cases. Built an end-to-end automated annotation workflow integrating QuPath with custom Groovy scripts, reducing manual processing time by 60% and enabling seamless data transfer from pathologists\u0026rsquo; annotations to AI training systems. Implemented YOLOv5 for real-time pathological slide screening, optimizing performance through custom data augmentation pipelines that expanded training datasets from 800 to 2,400 images while preserving critical pathological features. Created a distributed training framework using Horovod that accelerated model training by 3.9x, reducing cycle time from 3 weeks to 5 days while processing 1.5TB of medical imaging data weekly. Architected a multi-model AI system that combined outputs from tubule, mitosis, and nuclear pleomorphism detection models to automate Nottingham Grading for breast carcinoma, improving diagnostic accuracy from 80% to 92%. Engineered sliding window inference and efficient CUDA memory management techniques for whole-slide images (40,000×40,000 pixels), reducing GPU memory usage by 40% while maintaining inference speed. Developed comprehensive MLOps practices including Docker containerization, model versioning with DVC, experiment tracking via MLflow, and automated testing with pytest, ensuring reproducible and scalable AI deployment. Built an ensemble approach combining Faster R-CNN and YOLOv5 outputs, implementing gradCAM visualization to provide interpretable heatmaps of model decisions that increased pathologist trust in AI-assisted diagnosis. Published two research papers at prestigious medical imaging conferences, establishing novel methodologies for tubule detection and mitosis identification using deep learning approaches for breast cancer diagnosis. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/digital-clinics-research-and-services/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEngineered a sophisticated image classification system using Faster R-CNN for cancer cell detection in histopathological images, achieving 0.82 F1 score across 1,000 validated samples while optimizing model architecture for clinical efficiency.\u003c/li\u003e\n\u003cli\u003eDeveloped a custom segmentation pipeline with Detectron2 that precisely delineated tumor boundaries in high-resolution medical scans, implementing a specialized loss function that improved detection accuracy by 20% on challenging cases.\u003c/li\u003e\n\u003cli\u003eBuilt an end-to-end automated annotation workflow integrating QuPath with custom Groovy scripts, reducing manual processing time by 60% and enabling seamless data transfer from pathologists\u0026rsquo; annotations to AI training systems.\u003c/li\u003e\n\u003cli\u003eImplemented YOLOv5 for real-time pathological slide screening, optimizing performance through custom data augmentation pipelines that expanded training datasets from 800 to 2,400 images while preserving critical pathological features.\u003c/li\u003e\n\u003cli\u003eCreated a distributed training framework using Horovod that accelerated model training by 3.9x, reducing cycle time from 3 weeks to 5 days while processing 1.5TB of medical imaging data weekly.\u003c/li\u003e\n\u003cli\u003eArchitected a multi-model AI system that combined outputs from tubule, mitosis, and nuclear pleomorphism detection models to automate Nottingham Grading for breast carcinoma, improving diagnostic accuracy from 80% to 92%.\u003c/li\u003e\n\u003cli\u003eEngineered sliding window inference and efficient CUDA memory management techniques for whole-slide images (40,000×40,000 pixels), reducing GPU memory usage by 40% while maintaining inference speed.\u003c/li\u003e\n\u003cli\u003eDeveloped comprehensive MLOps practices including Docker containerization, model versioning with DVC, experiment tracking via MLflow, and automated testing with pytest, ensuring reproducible and scalable AI deployment.\u003c/li\u003e\n\u003cli\u003eBuilt an ensemble approach combining Faster R-CNN and YOLOv5 outputs, implementing gradCAM visualization to provide interpretable heatmaps of model decisions that increased pathologist trust in AI-assisted diagnosis.\u003c/li\u003e\n\u003cli\u003ePublished two research papers at prestigious medical imaging conferences, establishing novel methodologies for tubule detection and mitosis identification using deep learning approaches for breast cancer diagnosis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/digital-clinics-research-and-services/img-1.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Data Scientist"},{"content":"🔗 GitHub 🔗 Live Demo Description Developed a machine learning system that predicts Indian Premier League (IPL) cricket first innings scores with a Root Mean Square Error (RMSE) of approximately 20 runs using Ridge and Lasso regression models.\nEngineered a comprehensive data preprocessing pipeline that handles categorical features through one-hot encoding, addressing the complexity of team matchups in cricket analytics.\nImplemented temporal data splitting strategy by using pre-2017 matches for training and post-2017 matches for testing, ensuring robust model evaluation that mimics real-world prediction scenarios.\nApplied feature selection by filtering for consistent teams across seasons and removing irrelevant columns, improving model generalization and reducing noise in the prediction system.\nOptimized model hyperparameters using GridSearchCV with 5-fold cross-validation, systematically evaluating 12 different alpha values to find the optimal regularization strength.\nConducted comparative analysis between Ridge and Lasso regression models, with Ridge regression achieving slightly better performance (MSE: 403.2 vs 414.6) for this specific prediction task.\nCreated error distribution visualizations to understand prediction patterns, revealing normally distributed errors centered around zero with most predictions falling within ±20 runs.\nBuilt a Flask web application with an intuitive interface allowing users to select match conditions like batting team, bowling team, current score, and overs played to generate score predictions.\nDeployed the trained model as a web service using Heroku\u0026rsquo;s cloud platform, making the predictive analytics tool accessible to cricket fans and analysts worldwide.\nSerialized both Ridge and Lasso models using pickle, enabling fast loading and prediction without retraining, while maintaining the option to switch between models as needed.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/ipl-first-inning/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/IPL_First_Innings_Score\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-live-demo\"\u003e🔗 \u003ca href=\"https://ipl-score-predictor-app.herokuapp.com/\"\u003eLive Demo\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped a machine learning system that predicts Indian Premier League (IPL) cricket first innings scores with a Root Mean Square Error (RMSE) of approximately 20 runs using Ridge and Lasso regression models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEngineered a comprehensive data preprocessing pipeline that handles categorical features through one-hot encoding, addressing the complexity of team matchups in cricket analytics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented temporal data splitting strategy by using pre-2017 matches for training and post-2017 matches for testing, ensuring robust model evaluation that mimics real-world prediction scenarios.\u003c/p\u003e","title":"IPL First Innings Score Predictor"},{"content":"🔗 GitHub Description Developed an end-to-end license plate detection and recognition system using TensorFlow Object Detection API and EasyOCR, achieving high accuracy in real-time environments.\nImplemented transfer learning with SSD MobileNet v2 FPN-Lite architecture pre-trained on COCO dataset, fine-tuned for the specific task of license plate detection.\nCreated a comprehensive data pipeline including image collection, annotation, preprocessing, and TFRecord generation for efficient model training.\nOptimized model hyperparameters by modifying the pipeline configuration, including learning rate schedules, batch sizes, and anchor box settings for small object detection.\nEngineered a post-processing pipeline that automatically extracts detected license plate regions, applies OCR, and filters text results based on confidence thresholds.\nBuilt a real-time detection system using OpenCV that processes webcam feed, detects license plates, performs OCR, and logs results to CSV files with corresponding images.\nImplemented model export functionality to multiple formats (TensorFlow SavedModel, TFJS, TFLite) enabling deployment across different platforms and devices.\nCreated region-of-interest (ROI) filtering to improve OCR accuracy by analyzing the relative size and position of text within detected license plate regions.\nDesigned the system to automatically save detection results with unique identifiers for later auditing and verification purposes.\nLeveraged GPU acceleration through TensorFlow, enabling efficient training and near real-time inference performance on compatible hardware.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/number-plate-detection/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/Number-Plate-Detection\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped an end-to-end license plate detection and recognition system using TensorFlow Object Detection API and EasyOCR, achieving high accuracy in real-time environments.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented transfer learning with SSD MobileNet v2 FPN-Lite architecture pre-trained on COCO dataset, fine-tuned for the specific task of license plate detection.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreated a comprehensive data pipeline including image collection, annotation, preprocessing, and TFRecord generation for efficient model training.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOptimized model hyperparameters by modifying the pipeline configuration, including learning rate schedules, batch sizes, and anchor box settings for small object detection.\u003c/p\u003e","title":"Number Plate Detection \u0026 Recognition"},{"content":"Description Engineered a YOLOv5-based real-time object detection system for inventory management, creating a complete ML pipeline from data collection and annotation to deployment, achieving 90% accuracy in automated tracking and reducing manual counting time by 60%. Built a responsive web interface using Streamlit and integrated WebRTC for low-latency video processing, enabling real-time object recognition with less than 200ms latency while maintaining detection precision across varied lighting conditions. Developed an end-to-end data pipeline for ML model training, implementing custom data augmentation techniques that improved model generalization by 40% for inconsistent lighting and reflective surfaces common in storage environments. Created a portable radio simulator for military training using Python, WebRTC and socket programming, implementing real-time audio transmission with AES-256 encryption that reduced hardware dependency by 80% while maintaining secure communications. Designed and optimized a custom audio packet prioritization system that reduced transmission latency from 1.5 seconds to under 200ms, implementing Forward Error Correction algorithms that recovered up to 90% of lost packets in challenging network conditions. Implemented comprehensive database integration using SQLite with custom API endpoints, enabling automated inventory updates and alerts for low stock conditions that reduced stockouts and overstocking issues by 70%. Developed a cross-platform deployment architecture using Docker and AWS services (EC2, Lambda, S3), ensuring scalability while maintaining consistent performance across different operating environments. Engineered an advanced speech recognition system with PyTorch, achieving 95% accuracy when deployed in an army communication emulator, significantly enhancing training realism and combat readiness. Created detailed Software Requirement Specifications that effectively managed evolving client requirements, reducing scope creep and ensuring project stability while delivering on core objectives within tight timelines. Built a custom dataset creation and annotation pipeline using Roboflow, developing efficient labeling workflows that accelerated data preparation and enabled rapid model iterations to meet changing recognition requirements. ","permalink":"https://faseehahmed26.github.io/portfolio/experience/edgeforce-solutions/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEngineered a YOLOv5-based real-time object detection system for inventory management, creating a complete ML pipeline from data collection and annotation to deployment, achieving 90% accuracy in automated tracking and reducing manual counting time by 60%.\u003c/li\u003e\n\u003cli\u003eBuilt a responsive web interface using Streamlit and integrated WebRTC for low-latency video processing, enabling real-time object recognition with less than 200ms latency while maintaining detection precision across varied lighting conditions.\u003c/li\u003e\n\u003cli\u003eDeveloped an end-to-end data pipeline for ML model training, implementing custom data augmentation techniques that improved model generalization by 40% for inconsistent lighting and reflective surfaces common in storage environments.\u003c/li\u003e\n\u003cli\u003eCreated a portable radio simulator for military training using Python, WebRTC and socket programming, implementing real-time audio transmission with AES-256 encryption that reduced hardware dependency by 80% while maintaining secure communications.\u003c/li\u003e\n\u003cli\u003eDesigned and optimized a custom audio packet prioritization system that reduced transmission latency from 1.5 seconds to under 200ms, implementing Forward Error Correction algorithms that recovered up to 90% of lost packets in challenging network conditions.\u003c/li\u003e\n\u003cli\u003eImplemented comprehensive database integration using SQLite with custom API endpoints, enabling automated inventory updates and alerts for low stock conditions that reduced stockouts and overstocking issues by 70%.\u003c/li\u003e\n\u003cli\u003eDeveloped a cross-platform deployment architecture using Docker and AWS services (EC2, Lambda, S3), ensuring scalability while maintaining consistent performance across different operating environments.\u003c/li\u003e\n\u003cli\u003eEngineered an advanced speech recognition system with PyTorch, achieving 95% accuracy when deployed in an army communication emulator, significantly enhancing training realism and combat readiness.\u003c/li\u003e\n\u003cli\u003eCreated detailed Software Requirement Specifications that effectively managed evolving client requirements, reducing scope creep and ensuring project stability while delivering on core objectives within tight timelines.\u003c/li\u003e\n\u003cli\u003eBuilt a custom dataset creation and annotation pipeline using Roboflow, developing efficient labeling workflows that accelerated data preparation and enabled rapid model iterations to meet changing recognition requirements.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/edgeforce-solutions/images.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Data Scientist Intern"},{"content":"🔗 View the Tableau Dashboard Overview This interactive Tableau dashboard explores Foreign Direct Investment (FDI) data across multiple sectors in India. The goal is to identify patterns, highlight top sectors attracting the highest and lowest FDI inflows, and visualize year-over-year variations for strategic insights.\nKey Features Top \u0026amp; Bottom Sectors\nQuick identification of sectors receiving the highest and lowest FDI, enabling a snapshot view of investment trends. Historical Trends\nYearly FDI inflow comparison reveals how economic policies and global factors influence sector-specific growth or decline. Intuitive Visuals\nClear bar charts, line graphs, and tables provide easy navigation and interactivity, offering drill-downs into specific sectors. Actionable Insights\nPolicy makers, analysts, and business stakeholders can leverage these insights to guide strategic decisions on investment opportunities. Technology Stack Data Visualization: Tableau Public Data Source: FDI records from official repositories (e.g., RBI, Ministry of Commerce) Key Visual Elements: Bar charts for sector-wise comparison Line graphs for year-over-year trends Filter-based interactivity for deeper analysis Use Cases Government \u0026amp; Policy Makers: Understand which sectors need policy reforms or promotional efforts. Investors \u0026amp; Business Leaders: Identify lucrative sectors and forecast future opportunities. Researchers \u0026amp; Academics: Analyze the impact of global economic factors on FDI distribution in India. Explore the dashboard to discover how FDI shapes the economic landscape and informs key investment decisions.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/fdi-india/","summary":"\u003ch3 id=\"-view-the-tableau-dashboard\"\u003e🔗 \u003ca href=\"https://public.tableau.com/shared/4XJSW8GNG?:display_count=n\u0026amp;:origin=viz_share_link\"\u003eView the Tableau Dashboard\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis interactive Tableau dashboard explores Foreign Direct Investment (FDI) data across multiple sectors in India. The goal is to identify patterns, highlight top sectors attracting the highest and lowest FDI inflows, and visualize year-over-year variations for strategic insights.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTop \u0026amp; Bottom Sectors\u003c/strong\u003e\u003cbr\u003e\nQuick identification of sectors receiving the highest and lowest FDI, enabling a snapshot view of investment trends.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHistorical Trends\u003c/strong\u003e\u003cbr\u003e\nYearly FDI inflow comparison reveals how economic policies and global factors influence sector-specific growth or decline.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntuitive Visuals\u003c/strong\u003e\u003cbr\u003e\nClear bar charts, line graphs, and tables provide easy navigation and interactivity, offering drill-downs into specific sectors.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eActionable Insights\u003c/strong\u003e\u003cbr\u003e\nPolicy makers, analysts, and business stakeholders can leverage these insights to guide strategic decisions on investment opportunities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technology-stack\"\u003eTechnology Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eData Visualization:\u003c/strong\u003e Tableau Public\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Source:\u003c/strong\u003e FDI records from official repositories (e.g., RBI, Ministry of Commerce)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKey Visual Elements:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eBar charts for sector-wise comparison\u003c/li\u003e\n\u003cli\u003eLine graphs for year-over-year trends\u003c/li\u003e\n\u003cli\u003eFilter-based interactivity for deeper analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"use-cases\"\u003eUse Cases\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGovernment \u0026amp; Policy Makers:\u003c/strong\u003e Understand which sectors need policy reforms or promotional efforts.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInvestors \u0026amp; Business Leaders:\u003c/strong\u003e Identify lucrative sectors and forecast future opportunities.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResearchers \u0026amp; Academics:\u003c/strong\u003e Analyze the impact of global economic factors on FDI distribution in India.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExplore the dashboard to discover how FDI shapes the economic landscape and informs key investment decisions.\u003c/p\u003e","title":"Analyzing FDI in Various Sectors of India"},{"content":"🔗 GitHub Description Created a Deep Convolutional Generative Adversarial Network (DC-GAN) that generates high-quality, diverse anime character images. This project demonstrates advanced deep learning techniques for synthetic data generation using PyTorch. I developed custom data collection scripts to efficiently gather and preprocess large volumes of anime images, and implemented sophisticated model architectures to capture the unique stylistic elements of anime art.\nThe system features custom loss functions and training strategies that significantly improved image quality and diversity. The generated images maintain consistent anime-style features while exhibiting variety in character appearance, positioning this tool as a valuable resource for digital artists and animation studios seeking reference material or style inspiration.\n","permalink":"https://faseehahmed26.github.io/portfolio/projects/synthetic-anime-data-generator/","summary":"\u003ch3 id=\"-github\"\u003e🔗 \u003ca href=\"https://github.com/faseehahmed26/GANs/tree/main/GAN_ANIME_DS\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eCreated a Deep Convolutional Generative Adversarial Network (DC-GAN) that generates high-quality, diverse anime character images. This project demonstrates advanced deep learning techniques for synthetic data generation using PyTorch. I developed custom data collection scripts to efficiently gather and preprocess large volumes of anime images, and implemented sophisticated model architectures to capture the unique stylistic elements of anime art.\u003c/p\u003e\n\u003cp\u003eThe system features custom loss functions and training strategies that significantly improved image quality and diversity. The generated images maintain consistent anime-style features while exhibiting variety in character appearance, positioning this tool as a valuable resource for digital artists and animation studios seeking reference material or style inspiration.\u003c/p\u003e","title":"Synthetic Anime Data Generator"}]